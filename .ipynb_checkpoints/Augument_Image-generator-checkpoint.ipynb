{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation for object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw \n",
    "import PIL\n",
    "import torch\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torchvision.transforms.functional as F\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', \n",
    "              'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "              'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')\n",
    "label_map = {k: v+1 for v, k in enumerate(voc_labels)}\n",
    "#Inverse mapping\n",
    "rev_label_map = {v: k for k, v in label_map.items()}\n",
    "#Colormap for bounding box\n",
    "CLASSES = 20\n",
    "distinct_colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                   for i in range(CLASSES)]\n",
    "label_color_map  = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annot(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    boxes = list()\n",
    "    labels = list()\n",
    "    difficulties = list()\n",
    "    \n",
    "    for object in root.iter(\"object\"):\n",
    "        difficult = int(object.find(\"difficult\").text == \"1\")\n",
    "        label = object.find(\"name\").text.lower().strip()\n",
    "        if label in label_map:\n",
    "\n",
    "            bbox =  object.find(\"bndbox\")\n",
    "            xmin = int(bbox.find(\"xmin\").text)\n",
    "            ymin = int(bbox.find(\"ymin\").text)\n",
    "            xmax = int(bbox.find(\"xmax\").text)\n",
    "            ymax = int(bbox.find(\"ymax\").text)\n",
    "\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            labels.append(label_map[label])\n",
    "            difficulties.append(difficult)\n",
    "\n",
    "    return {\"boxes\": boxes, \"labels\": labels, \"difficulties\": difficulties}\n",
    "\n",
    "def draw_PIL_image(image, boxes, labels):\n",
    "    '''\n",
    "        Draw PIL image\n",
    "        image: A PIL image\n",
    "        labels: A tensor of dimensions (#objects,)\n",
    "        boxes: A tensor of dimensions (#objects, 4)\n",
    "    '''\n",
    "    if type(image) != PIL.Image.Image:\n",
    "        image = F.to_pil_image(image)\n",
    "    new_image = image.copy()\n",
    "    labels = labels.tolist()\n",
    "    draw = ImageDraw.Draw(new_image)\n",
    "    boxes = boxes.tolist()\n",
    "    for i in range(len(boxes)):\n",
    "        draw.rectangle(xy= boxes[i], outline= label_color_map[rev_label_map[labels[i]]])\n",
    "    \n",
    "    display(new_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # BACK TO ANNOTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_xml_annotation(new_ann_path, image_filename, boxes, labels, difficulties):\n",
    "    # Create a new XML element tree\n",
    "    root = ET.Element('annotation')\n",
    "    filename = ET.SubElement(root, 'filename')\n",
    "    filename.text = image_filename\n",
    "\n",
    "    # Iterate through objects and add annotation for each\n",
    "    for i in range(len(boxes)):\n",
    "        object_elem = ET.SubElement(root, 'object')\n",
    "        \n",
    "        label_idx = labels[i]\n",
    "        label_name = rev_label_map[label_idx]  # Assuming rev_label_map is defined\n",
    "        difficult = '1' if difficulties[i] == 1 else '0'\n",
    "        \n",
    "        name_elem = ET.SubElement(object_elem, 'name')\n",
    "        name_elem.text = label_name\n",
    "        \n",
    "        difficult_elem = ET.SubElement(object_elem, 'difficult')\n",
    "        difficult_elem.text = difficult\n",
    "        \n",
    "        bbox_elem = ET.SubElement(object_elem, 'bndbox')\n",
    "        xmin_elem = ET.SubElement(bbox_elem, 'xmin')\n",
    "        xmin_elem.text = str(int(boxes[i][0]))\n",
    "        ymin_elem = ET.SubElement(bbox_elem, 'ymin')\n",
    "        ymin_elem.text = str(int(boxes[i][1]))\n",
    "        xmax_elem = ET.SubElement(bbox_elem, 'xmax')\n",
    "        xmax_elem.text = str(int(boxes[i][2]))\n",
    "        ymax_elem = ET.SubElement(bbox_elem, 'ymax')\n",
    "        ymax_elem.text = str(int(boxes[i][3]))\n",
    "\n",
    "    # Create and save the new XML annotation\n",
    "    tree = ET.ElementTree(root)\n",
    "    tree.write(new_ann_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUGUMENTED IMAGE GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1:) Contrast and Brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Augumentation does not change the location of bounding box, so exact ann file is copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    def Adjust_contrast(image):\n",
    "        return F.adjust_contrast(image,2)\n",
    "    new_image = Adjust_contrast(image)\n",
    "    \n",
    "    def Adjust_brightness(image):\n",
    "        return F.adjust_brightness(image, 2)\n",
    "    new_image = Adjust_brightness(new_image)\n",
    "        \n",
    "        \n",
    "    #draw_PIL_image(new_image, boxes, labels)\n",
    "    \n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_Contrast_and_brightness.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_Contrast_and_brightness.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename)\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename)\n",
    "    #shutil.copy(ann_path, new_ann_path)\n",
    "    #generate_xml_annotation(new_ann_path, new_image_filename, boxes, labels, difficulties)\n",
    "    \n",
    "    boxes_list = boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHTING NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    def lighting_noise(image):\n",
    "        new_image = image\n",
    "        perms = ((0, 1, 2), (0, 2, 1), (1, 0, 2), \n",
    "             (1, 2, 0), (2, 0, 1), (2, 1, 0))\n",
    "        swap = perms[random.randint(0, len(perms)- 1)]\n",
    "        new_image = F.to_tensor(new_image)\n",
    "        new_image = new_image[swap, :, :]\n",
    "        new_image = F.to_pil_image(new_image)\n",
    "        return new_image\n",
    "\n",
    "    \n",
    "    \n",
    "    new_image = lighting_noise(image)\n",
    "    \n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_Lighting_noise.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_Lighting_noise.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename) # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename)  # Ouput Destination\n",
    "    shutil.copy(ann_path, new_ann_path)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horizontal Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def horizontal_flip(image, boxes):\n",
    "        new_image = F.hflip(image)\n",
    "\n",
    "        #flip boxes \n",
    "        new_boxes = boxes.clone()\n",
    "        new_boxes[:, 0] = image.width - boxes[:, 0]\n",
    "        new_boxes[:, 2] = image.width - boxes[:, 2]\n",
    "        new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
    "        return new_image, new_boxes\n",
    "\n",
    "\n",
    "    new_image, new_boxes = horizontal_flip(image, boxes)\n",
    "    draw_PIL_image(new_image, new_boxes, labels)\n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_horizontal_flip.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_horizontal_flip.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename) # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename) # Ouput Destination\n",
    "\n",
    "    \n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "    \n",
    "    def vertical_flip(image, boxes):\n",
    "\n",
    "        # Flip the image vertically\n",
    "        new_image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "\n",
    "        # Flip the bounding boxes\n",
    "        new_boxes = boxes.clone()\n",
    "        new_boxes[:, 1] = image.height - boxes[:, 3]\n",
    "        new_boxes[:, 3] = image.height - boxes[:, 1]\n",
    "\n",
    "        return new_image, new_boxes\n",
    "\n",
    "\n",
    "    new_image, new_boxes = vertical_flip(image, boxes)\n",
    "    draw_PIL_image(new_image, new_boxes, labels)\n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_vertical_flip.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_vertical_lip.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename)  # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename)  # Ouput Destination\n",
    "\n",
    "    \n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    \n",
    "    def intersect(boxes1, boxes2):\n",
    "        \n",
    "            #Find intersection of every box combination between two sets of box\n",
    "            #boxes1: bounding boxes 1, a tensor of dimensions (n1, 4)\n",
    "            #boxes2: bounding boxes 2, a tensor of dimensions (n2, 4)\n",
    "\n",
    "            #Out: Intersection each of boxes1 with respect to each of boxes2, \n",
    "                 #a tensor of dimensions (n1, n2)\n",
    "        \n",
    "        n1 = boxes1.size(0)\n",
    "        n2 = boxes2.size(0)\n",
    "        max_xy =  torch.min(boxes1[:, 2:].unsqueeze(1).expand(n1, n2, 2),\n",
    "                            boxes2[:, 2:].unsqueeze(0).expand(n1, n2, 2))\n",
    "\n",
    "        min_xy = torch.max(boxes1[:, :2].unsqueeze(1).expand(n1, n2, 2),\n",
    "                           boxes2[:, :2].unsqueeze(0).expand(n1, n2, 2))\n",
    "        inter = torch.clamp(max_xy - min_xy , min=0)  # (n1, n2, 2)\n",
    "        return inter[:, :, 0] * inter[:, :, 1]  #(n1, n2)\n",
    "    def find_IoU(boxes1, boxes2):\n",
    "        \n",
    "            #Find IoU between every boxes set of boxes \n",
    "            #boxes1: a tensor of dimensions (n1, 4) (left, top, right , bottom)\n",
    "            #boxes2: a tensor of dimensions (n2, 4)\n",
    "\n",
    "            #Out: IoU each of boxes1 with respect to each of boxes2, a tensor of \n",
    "                 #dimensions (n1, n2)\n",
    "\n",
    "            #Formula: \n",
    "            #(box1 ∩ box2) / (box1 u box2) = (box1 ∩ box2) / (area(box1) + area(box2) - (box1 ∩ box2 ))\n",
    "        \n",
    "        inter = intersect(boxes1, boxes2)\n",
    "        area_boxes1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "        area_boxes2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "\n",
    "        area_boxes1 = area_boxes1.unsqueeze(1).expand_as(inter) #(n1, n2)\n",
    "        area_boxes2 = area_boxes2.unsqueeze(0).expand_as(inter)  #(n1, n2)\n",
    "        union = (area_boxes1 + area_boxes2 - inter)\n",
    "        return inter / union\n",
    "    \n",
    "    def random_crop(image, boxes, labels, difficulties):\n",
    "    \n",
    "        #image: A PIL image\n",
    "        #boxes: Bounding boxes, a tensor of dimensions (#objects, 4)\n",
    "        #labels: labels of object, a tensor of dimensions (#objects)\n",
    "        #difficulties: difficulties of detect object, a tensor of dimensions (#objects)\n",
    "        \n",
    "        #Out: cropped image , new boxes, new labels, new difficulties\n",
    "\n",
    "        if type(image) == PIL.Image.Image:\n",
    "            image = F.to_tensor(image)\n",
    "        original_h = image.size(1)\n",
    "        original_w = image.size(2)\n",
    "\n",
    "        while True:\n",
    "            mode = random.choice([0.1, 0.3, 0.5, 0.9, None])\n",
    "\n",
    "            if mode is None:\n",
    "                return image, boxes, labels, difficulties\n",
    "\n",
    "            new_image = image\n",
    "            new_boxes = boxes\n",
    "            new_difficulties = difficulties\n",
    "            new_labels = labels\n",
    "            for _ in range(50):\n",
    "                # Crop dimensions: [0.3, 1] of original dimensions\n",
    "                new_h = random.uniform(0.3*original_h, original_h)\n",
    "                new_w = random.uniform(0.3*original_w, original_w)\n",
    "\n",
    "                # Aspect ratio constraint b/t .5 & 2\n",
    "                if new_h/new_w < 0.5 or new_h/new_w > 2:\n",
    "                    continue\n",
    "\n",
    "                #Crop coordinate\n",
    "                left = random.uniform(0, original_w - new_w)\n",
    "                right = left + new_w\n",
    "                top = random.uniform(0, original_h - new_h)\n",
    "                bottom = top + new_h\n",
    "                crop = torch.FloatTensor([int(left), int(top), int(right), int(bottom)])\n",
    "\n",
    "                # Calculate IoU  between the crop and the bounding boxes\n",
    "                overlap = find_IoU(crop.unsqueeze(0), boxes) #(1, #objects)\n",
    "                overlap = overlap.squeeze(0)\n",
    "                # If not a single bounding box has a IoU of greater than the minimum, try again\n",
    "                if overlap.max().item() < mode:\n",
    "                    continue\n",
    "\n",
    "                #Crop\n",
    "                new_image = image[:, int(top):int(bottom), int(left):int(right)] #(3, new_h, new_w)\n",
    "\n",
    "                #Center of bounding boxes\n",
    "                center_bb = (boxes[:, :2] + boxes[:, 2:])/2.0\n",
    "\n",
    "                #Find bounding box has been had center in crop\n",
    "                center_in_crop = (center_bb[:, 0] >left) * (center_bb[:, 0] < right\n",
    "                                 ) *(center_bb[:, 1] > top) * (center_bb[:, 1] < bottom)    #( #objects)\n",
    "\n",
    "                if not center_in_crop.any():\n",
    "                    continue\n",
    "\n",
    "                #take matching bounding box\n",
    "                new_boxes = boxes[center_in_crop, :]\n",
    "\n",
    "                #take matching labels\n",
    "                new_labels = labels[center_in_crop]\n",
    "\n",
    "                #take matching difficulities\n",
    "                new_difficulties = difficulties[center_in_crop]\n",
    "\n",
    "                #Use the box left and top corner or the crop's\n",
    "                new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])\n",
    "\n",
    "                #adjust to crop\n",
    "                new_boxes[:, :2] -= crop[:2]\n",
    "\n",
    "                new_boxes[:, 2:] = torch.min(new_boxes[:, 2:],crop[2:])\n",
    "\n",
    "                #adjust to crop\n",
    "                new_boxes[:, 2:] -= crop[:2]\n",
    "\n",
    "                return F.to_pil_image(new_image), new_boxes, new_labels, new_difficulties\n",
    "\n",
    "\n",
    "    new_image,new_boxes, new_labels, new_difficulties = random_crop(image, boxes,labels, difficulties)\n",
    "    draw_PIL_image(new_image, new_boxes, new_labels)\n",
    "\n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_random_crop.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_random_crop.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename) # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename) # Ouput Destination\n",
    "     \n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = new_labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotate only bouding box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    \n",
    "    def rotate_only_bboxes(image, boxes, angle):\n",
    "        new_image = image.copy()\n",
    "        new_image = F.to_tensor(new_image)\n",
    "        for i in range(boxes.shape[0]):\n",
    "            x_min, y_min, x_max, y_max = map(int, boxes[i,:].tolist())\n",
    "            bbox = new_image[:,  y_min:y_max+1, x_min:x_max+1]\n",
    "            bbox = F.to_pil_image(bbox)\n",
    "            bbox = bbox.rotate(angle)\n",
    "\n",
    "            new_image[:,y_min:y_max+1, x_min:x_max+1] = F.to_tensor(bbox)\n",
    "        return F.to_pil_image(new_image)\n",
    "    \n",
    "    new_image = rotate_only_bboxes(image, boxes, 5)\n",
    "\n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_rotate_bbox.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_rotate_bbox.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename) # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename) # Ouput Destination\n",
    "    \n",
    "    boxes_list = boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cutout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "\n",
    "    def cutout(image, boxes, labels, fill_val= 0, bbox_remove_thres= 0.4):\n",
    "        if type(image) == PIL.Image.Image:\n",
    "            image = F.to_tensor(image)\n",
    "        original_h = image.size(1)\n",
    "        original_w = image.size(2)\n",
    "        original_channel = image.size(0)\n",
    "\n",
    "        new_image = image\n",
    "        new_boxes = boxes\n",
    "        new_labels = labels\n",
    "\n",
    "        for _ in range(50):\n",
    "            #Random cutout size: [0.15, 0.5] of original dimension\n",
    "            cutout_size_h = random.uniform(0.15*original_h, 0.5*original_h)\n",
    "            cutout_size_w = random.uniform(0.15*original_w, 0.5*original_w)\n",
    "\n",
    "            #Random position for cutout\n",
    "            left = random.uniform(0, original_w - cutout_size_w)\n",
    "            right = left + cutout_size_w\n",
    "            top = random.uniform(0, original_h - cutout_size_h)\n",
    "            bottom = top + cutout_size_h\n",
    "            cutout = torch.FloatTensor([int(left), int(top), int(right), int(bottom)])\n",
    "\n",
    "            #Calculate intersect between cutout and bounding boxes\n",
    "            overlap_size = intersect(cutout.unsqueeze(0), boxes)\n",
    "            area_boxes = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "            ratio = overlap_size / area_boxes\n",
    "            #If all boxes have Iou greater than bbox_remove_thres, try again\n",
    "            if ratio.min().item() > bbox_remove_thres:\n",
    "                continue\n",
    "\n",
    "            cutout_arr = torch.full((original_channel,int(bottom) - int(top),int(right) - int(left)), fill_val)\n",
    "            new_image[:, int(top):int(bottom), int(left):int(right)] = cutout_arr\n",
    "\n",
    "            #Create new boxes and labels\n",
    "            boolean = ratio < bbox_remove_thres\n",
    "\n",
    "            new_boxes = boxes[boolean[0], :]\n",
    "\n",
    "            new_labels = labels[boolean[0]]\n",
    "\n",
    "            return F.to_pil_image(new_image), new_boxes, new_labels\n",
    "\n",
    "    new_image,new_boxes, new_labels = cutout(image, boxes,labels)\n",
    "    draw_PIL_image(new_image, new_boxes, new_labels)\n",
    "\n",
    "    #draw_PIL_image(new_image, boxes, labels)\n",
    "    \n",
    "    \n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_Occlusion.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_Occlusion.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename)  # Ouput Destination\n",
    "    new_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename)  # Ouput Destination\n",
    "\n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = new_labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    def gaussian_noise(image, boxes, mean=0, std=20):\n",
    "\n",
    "        # Convert PIL image to NumPy array\n",
    "        image_np = np.array(image)\n",
    "\n",
    "        # Generate Gaussian noise\n",
    "        noise = np.random.normal(mean, std, image_np.shape).astype(np.uint8)\n",
    "\n",
    "        # Add noise to the image\n",
    "        noisy_image_np = np.clip(image_np + noise, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Convert back to PIL format\n",
    "        noisy_image = Image.fromarray(noisy_image_np)\n",
    "\n",
    "        return noisy_image, boxes\n",
    "        \n",
    "    noisy_image, new_boxes = gaussian_noise(image, boxes)\n",
    "\n",
    "\n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_gaussian_noise.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_gaussian_noise.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename)  # Ouput Destination\n",
    "    noisy_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename)  # Ouput Destination\n",
    "\n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)    \n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = r\"C:\\Users\\Garvit\\Downloads\\Dataset\"        #Path to DataSet\n",
    "\n",
    "# Get a list of image files in the folder\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.jpg')]\n",
    "ann_files= [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "\n",
    "for image_file, ann_file in zip(image_files,ann_files):  # BOTH FILES ARE READ TOGETHER\n",
    "    image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', image_file)\n",
    "    ann_path= os.path.join(r'C:\\Users\\Garvit\\Downloads\\Dataset', ann_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert(\"RGB\")\n",
    "    objects= parse_annot(ann_path)\n",
    "    boxes = torch.FloatTensor(objects['boxes'])\n",
    "    labels = torch.LongTensor(objects['labels']) \n",
    "    difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "    \n",
    "  # APPLYING FILTERS/AUGUMENTATION METHODS\n",
    "    def motion_blur(image, boxes, kernel_size=15, angle=45):\n",
    "\n",
    "        # Convert PIL image to OpenCV format\n",
    "        image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # Create a motion blur kernel\n",
    "        kernel = np.zeros((kernel_size, kernel_size))\n",
    "        kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)\n",
    "        kernel = kernel / kernel_size\n",
    "\n",
    "        # Apply motion blur\n",
    "        motion_blurred_image_cv = cv2.filter2D(image_cv, -1, kernel)\n",
    "\n",
    "        # Convert back to PIL format\n",
    "        motion_blurred_image = Image.fromarray(cv2.cvtColor(motion_blurred_image_cv, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        return motion_blurred_image, boxes\n",
    "        \n",
    "    blurred_image, new_boxes = motion_blur(image, boxes, kernel_size=15, angle=45)\n",
    "\n",
    "    # SAVING THE NEW FILE\n",
    "    new_image_filename = os.path.splitext(image_file)[0] + \"_motion_blur.jpg\"\n",
    "    new_ann_filename = os.path.splitext(ann_file)[0] + \"_motion_blur.xml\"\n",
    "\n",
    "    # Save the processed image\n",
    "    new_image_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_image_filename)   # Ouput Destination\n",
    "    blurred_image.save(new_image_path)\n",
    "\n",
    "    # Generate and save the new XML annotation\n",
    "    new_ann_path = os.path.join(r'C:\\Users\\Garvit\\Downloads\\Test', new_ann_filename) # Ouput Destination\n",
    "\n",
    "    boxes_list = new_boxes.tolist()\n",
    "    labels_list = labels.tolist()\n",
    "    difficulties_list = difficulties.tolist()\n",
    "    generate_xml_annotation(new_ann_path, new_image_filename, boxes_list, labels_list, difficulties_list)\n",
    "    #break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Transformed Image + Bbox \n",
    "\n",
    "#### -> Write break at the end of code, generate one file each (if there are more than one,else loop will stop automatically)\n",
    "#### -> Replace the paths of file and check if the image and annotation has transformed and are fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(r'Test.img')    #Transformed Image File Path\n",
    "image = image.convert(\"RGB\")\n",
    "objects= parse_annot(r'Test.xml')  #Transformed Annotation File Path\n",
    "boxes = torch.FloatTensor(objects['boxes'])\n",
    "labels = torch.LongTensor(objects['labels']) \n",
    "difficulties = torch.ByteTensor(objects['difficulties'])\n",
    "draw_PIL_image(image, boxes, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
